{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Messaging Bot\n",
    "\n",
    "## Download database from S3, update the data, and upload database to S3\n",
    "\n",
    "1. Create user agent on Reddit API \n",
    "2. Extract hot posts and top posts creator IDs from Reddit API\n",
    "3. Combine the two creator ID lists and remove the duplicates\n",
    "4. Check the creator IDs if they already exists in the current creator ID database. If not, add them to the shuffle list\n",
    "5. Shuffle the list to make sure every creator have the same probability to be chosen\n",
    "6. Divide the list into three groups\n",
    "7. Send different messages to three groups\n",
    "8. Save new creator IDs in our creator ID database\n",
    "9. upload the database to AWS S3\n",
    "\n",
    "Todo:\n",
    "shuffle list and send msgs to creators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "# s3.create_bucket(Bucket='top-hot-posts-creator-ids',\n",
    "#     CreateBucketConfiguration={'LocationConstraint': 'us-west-1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file('top-hot-posts-creator-ids','author_id.pkl', 'author_id.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from praw.models import MoreComments\n",
    "\n",
    "# Enter the message title and body\n",
    "title1 = 'Hello'\n",
    "title2 = 'Hi'\n",
    "title3 = 'How are you?'\n",
    "\n",
    "body1 = ''\n",
    "body2 = ''\n",
    "body3 = ''\n",
    "\n",
    "# Enter your client id, secret and user agent\n",
    "reddit = praw.Reddit(client_id = 'k_irONQxgcqWFg', \n",
    "                client_secret = 'A6udBWn-8PXi2p7X34K7HT5THiA', \n",
    "                user_agent = 'Test',\n",
    "                username='AddyZhang',\n",
    "                password='Haldis199444')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit groups that Drew wants us to look at\n",
    "Reddit_groups = ['depression', 'anxiety', 'OCD', 'socialanxiety', 'panicdisorder']\n",
    "\n",
    "# Create a dataframe to store text data\n",
    "def hot_posts_to_df():\n",
    "    # Write a loop to put hot post from all groups and their key info into one dataframe\n",
    "    posts = []\n",
    "    for group in Reddit_groups:\n",
    "        mental_subreddit = reddit.subreddit(group)\n",
    "        for post in mental_subreddit.hot(limit=10): # you can change the top number of posts \n",
    "\n",
    "            posts.append([group, post.title, post.url, post.id, post.author])\n",
    "\n",
    "    posts = pd.DataFrame(posts, columns = ['group','title', 'url','post_id','author']) \n",
    "    \n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_posts_to_df():\n",
    "    # Write a loop to put hot post from all groups and their key info into one dataframe\n",
    "    posts = []\n",
    "    for group in Reddit_groups:\n",
    "        mental_subreddit = reddit.subreddit(group)\n",
    "        for post in mental_subreddit.top('day', limit=10): # you can change the top number of posts \n",
    "\n",
    "            posts.append([group, post.title, post.url, post.id, post.author])\n",
    "\n",
    "    posts = pd.DataFrame(posts, columns = ['group','title', 'url','post_id','author']) \n",
    "    \n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_list(l1,l2):\n",
    "    l3 = l1+l2\n",
    "    return list(set(l3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_pickle(file_name):  \n",
    "    \n",
    "    with open(f'{file_name}.pkl', 'rb') as f:\n",
    "        author_id_db = pickle.load(f)\n",
    "    \n",
    "    return author_id_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_pickle(file_name, list_to_pickle):\n",
    "    \n",
    "    with open(f'{file_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(list_to_pickle, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message(user_id, title, body):\n",
    "    \"\"\"\n",
    "    Send private messages to redditor with the title and body\n",
    "    \"\"\"\n",
    "    reddit.redditor(f'{user_id}').message(f'{title}', f'{body}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shuffle_list(list_unsf):\n",
    "    \"\"\"\n",
    "    Shuffle list to make sure everyone that has the same probabilty to be selected\n",
    "    \"\"\"\n",
    "    random.shuffle(list_unsf)\n",
    "    return list_unsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_group(list_sf):\n",
    "    k = int(len(list_sf)/3) # divide into three groups\n",
    "    group1 = list_sf[:k]\n",
    "    group2 = list_sf[k:2*k]\n",
    "    group3 = list_sf[2*k:]\n",
    "    return group1, group2, group3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    hot_posts = hot_posts_to_df()\n",
    "    hot_posts.author = hot_posts.author.astype('str') # change dtypes\n",
    "    hot_author_id_list = hot_posts.author.tolist()\n",
    "    \n",
    "    top_posts = top_posts_to_df()\n",
    "    top_posts.author = top_posts.author.astype('str') # change dtypes\n",
    "    top_author_id_list = top_posts.author.tolist()\n",
    "    \n",
    "    author_id_list = combine_list(hot_author_id_list, top_author_id_list) # combine and remove duplicates\n",
    "    file_name = 'author_id'\n",
    "    author_id_db = load_pickle(file_name)\n",
    "    \n",
    "#     author_id_rs = [] # need this list for random sampling\n",
    "    for author_id in author_id_list:\n",
    "        if author_id not in author_id_db:\n",
    "            \n",
    "#             author_id_rs.append(author_id) # append the new author_id in our random sampling list\n",
    "            author_id_db.append(author_id) # append the new author_id to our database list \n",
    "    \n",
    "    \n",
    "#     author_id_sf = shuffle_list(author_id_rs) # shuffle the creator id list\n",
    "\n",
    "#     group1, group2, group3 = divide_group(author_id_sf) # divide into three groups\n",
    "    \n",
    "    # send msgs to three group users\n",
    "#     for user_id in group1:\n",
    "#         message(user_id,title1,body1)\n",
    "    \n",
    "#     for user_id in group2:\n",
    "#         message(user_id,title2,body2)\n",
    "    \n",
    "#     for user_id in group3:\n",
    "#         message(user_id,title3,body3)\n",
    "\n",
    "#     print(group1,group2,group3)\n",
    "    save_as_pickle(file_name, author_id_db)\n",
    "    s3.upload_file('author_id.pkl','top-hot-posts-creator-ids', 'author_id.pkl')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xpanda16', 'Mental_senstrias', 'pvdilla123', 'madsadchadglad', 'Canucks_ZinccxFan', 'xSly_Foxx', 'circinia', 'Mumbawobz', 'tetrisphere69', 'Gerbilll', 'r36ei', 'patientlips', 'Bodhicahya', 'magentatroops', 'Wysa_ai', 'ilovemalakwaleed', 'moonstone28', 'codbla69', 't0niidk', 'ihateworkingandlife', 'invaderxanny', 'p0Oferdogmam', 'Vehicle_Voltron', 'SolomonNightrider', 'clappingenballs', 'szman135', 'Nervous-Tale', 'alyssaoftheeast', 'ImetalheadBGI', 'KikilooRose', 'GiltVoices', 'RedSoxCeltics', 'Yacinekoukou', 'socialanimal_us', 'suicidalsadgirl', 'MaterialSquid', 'princess-kelly', 'originalmoon', 'TeodrosKing', 'angelshum10', 'tinylilcaterpillar', 'blazdncnfsd', 'DrJanaScrivani', 'FatherSteph', 'Movingforwardtimes', 'OhSheGlows', 'twentysixants', 'MuchRedditLessTime', 'transparentfears', 'grozdeto', 'p3ras1', 'elabanana_', 'wilmygirl22', 'pikachuuuuu', 'Raven-OA_', 'SidTheNerd', 'throwaway83426', 'Weaselur', 'cezzatron', 'jacobamcdonald', 'haha_ur_mom_gay_haha', 'loveandsqualor4', 'lequestionmaster', 'SQLwitch', 'emzluv', 'some-rndom-ass-kid', 'Sydney142', 'cutthewire', 'availableusername94', 'imiximix', 'bitchassery', 'noooothankssss', 'its_nehaha', 'queessi', 'TheRealJola', 'Mucina_Mugwort', 'CanOfBirds']\n"
     ]
    }
   ],
   "source": [
    "print(author_id_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
